{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCXr2GNOIlgEugQ+xvl3cF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardocappi/Machine_Learning_Course/blob/main/Lec_9_Back_Propagation_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Back-propagation from scratch###\n",
        "**Riccardo Cappi, 2073768**\n",
        "\n",
        "In this notebook i implemented the Back-Propagation algorithm shown in Lecture 9 extending the derivation to more than one hidden layer.\n",
        "\n",
        "I tested the algorithm trying to replicate the network given on slide 15 (Encoder of the identity function)"
      ],
      "metadata": {
        "id": "kKJw5Rhwhd-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ceJTHqIpJUPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network components###\n"
      ],
      "metadata": {
        "id": "X6n0bP6FlO5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piK-X0fCo0KK"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class HiddenUnit:\n",
        "  def __init__(self, weights):\n",
        "    self.weights = weights[1:]\n",
        "    self.delta_w = np.zeros(len(weights[1:]))\n",
        "    self.delta = 0\n",
        "    self.bias = weights[0]\n",
        "  \n",
        "  def net(self, input):\n",
        "    y = np.dot(self.weights,input) + self.bias\n",
        "    return y\n",
        "\n",
        "  def update_weights(self, lr):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.weights[i] += lr*self.delta_w[i] \n",
        "\n",
        "    self.bias += lr*self.delta\n",
        "\n",
        "\n",
        "class HiddenLayer:\n",
        "  def __init__(self, input_size, dim):\n",
        "    self.hidden_units = []\n",
        "    for i in range(dim):\n",
        "      self.hidden_units.append( HiddenUnit(np.random.uniform(low = -0.05, high = 0.05,size = input_size + 1)) )\n",
        "  \n",
        "  def get_hidden_layer_nets(self, input):\n",
        "    # Computes the output of each neuron on this layer\n",
        "    out = []\n",
        "    for h in self.hidden_units:\n",
        "      out.append(h.net(input))\n",
        "    return np.asarray(out)\n",
        "\n",
        "\n",
        "def backpropagation(network, output_of_each_layer, lr, t, x):\n",
        "  for i in reversed(range(len(network))):\n",
        "    hidden_layer = network[i]\n",
        "    layer_outputs = output_of_each_layer[i]\n",
        "    # If we are iterating through the first hidden layer the input will be the input layer, otherwise it will be the output of the previous layer\n",
        "    layer_input = x if i == 0 else output_of_each_layer[i-1]\n",
        "    for j, j_unit in enumerate(hidden_layer.hidden_units):\n",
        "      j_output = layer_outputs[j]\n",
        "      is_output_layer = i == (len(network) - 1)\n",
        "      j_unit.delta = j_output * (1.0 - j_output) * get_error(j_output,is_output_layer,network,i,t,j) \n",
        "      for k in range(len(j_unit.delta_w)):\n",
        "        j_unit.delta_w[k] = j_unit.delta * layer_input[k]\n",
        "\n",
        "  # Update weights\n",
        "  for hl in network:\n",
        "    for unit in hl.hidden_units:\n",
        "      unit.update_weights(lr)\n",
        "\n",
        "  return network\n",
        "\n",
        "\n",
        "def get_error(output, is_output_layer, network, i,t,j):\n",
        "  error = 0.0\n",
        "  if is_output_layer:\n",
        "    error = t[j] - output\n",
        "  else:\n",
        "    for unit in network[i+1].hidden_units:\n",
        "      error += unit.weights[j] * unit.delta \n",
        "  return error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The forward function applies the sigmoid activation to each neuron output\n",
        "def forward(network, input_layer):\n",
        "  out = []\n",
        "  input = input_layer\n",
        "  for hl in network:\n",
        "    values = sigmoid(hl.get_hidden_layer_nets(input))\n",
        "    out.append(values)\n",
        "    input = values\n",
        "  return out\n",
        "\n",
        "def train(network, X, y, lr, steps):\n",
        "  S = list(zip(X,y))\n",
        "  print('Learning started')\n",
        "  for _ in range(steps):\n",
        "    for x,t in S:\n",
        "      output = forward(network, x)\n",
        "      network = backpropagation(network, output, lr, t,x)\n",
        "\n",
        "  print('Learning ended\\n')\n",
        "  return network\n"
      ],
      "metadata": {
        "id": "1qrFfTVoBgnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test on Identity function###\n",
        "\n",
        "I built a network following the architecture of the encoder shown il Lecture 9 (slide 15) and tried to learn the Identity Function (8x8)"
      ],
      "metadata": {
        "id": "1miWEASXDL_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = np.identity(8)\n",
        "y = np.identity(8)\n",
        "\n",
        "h1 = HiddenLayer(8,3)\n",
        "out = HiddenLayer(3, 8)\n",
        "network = [h1,out]\n",
        "\n",
        "network = train(network,X,y,0.2,5000)\n",
        "print('Input\\t\\t\\t\\t\\t', 'Hidden Values\\t\\t\\t\\t','Output')\n",
        "for x in X:\n",
        "  y,z = forward(network,x)\n",
        "  print(x, '\\t\\t'+ str([round(y_i,2) for y_i in y])+'\\t\\t\\t', [0 if z_i <0.5 else 1 for z_i in z])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afT9BdC6Vn5p",
        "outputId": "48ce0806-16fd-4fa3-8b0c-487bf6f47805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning started\n",
            "Learning ended\n",
            "\n",
            "Input\t\t\t\t\t Hidden Values\t\t\t\t Output\n",
            "[1. 0. 0. 0. 0. 0. 0. 0.] \t\t[0.81, 0.01, 0.95]\t\t\t [1, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0. 1. 0. 0. 0. 0. 0. 0.] \t\t[0.88, 0.98, 0.02]\t\t\t [0, 1, 0, 0, 0, 0, 0, 0]\n",
            "[0. 0. 1. 0. 0. 0. 0. 0.] \t\t[0.97, 0.11, 0.03]\t\t\t [0, 0, 1, 0, 0, 0, 0, 0]\n",
            "[0. 0. 0. 1. 0. 0. 0. 0.] \t\t[0.01, 0.64, 0.01]\t\t\t [0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[0. 0. 0. 0. 1. 0. 0. 0.] \t\t[0.05, 0.99, 0.74]\t\t\t [0, 0, 0, 0, 1, 0, 0, 0]\n",
            "[0. 0. 0. 0. 0. 1. 0. 0.] \t\t[0.04, 0.01, 0.19]\t\t\t [0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0.] \t\t[0.98, 0.95, 0.99]\t\t\t [0, 0, 0, 0, 0, 0, 1, 0]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1.] \t\t[0.01, 0.27, 0.98]\t\t\t [0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Perceptron as a particular case of MultiLayer Network###\n",
        "I created a Perceptron using the previous code and learned the OR function"
      ],
      "metadata": {
        "id": "3HScDajksb_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "OR = lambda x1, x2, x3: x1 or x2 or x3\n",
        "data = list(product((0,1),repeat=3))\n",
        "data = np.array(data)\n",
        "t = [OR(*s) for s in data]\n",
        "\n",
        "t = [[t_i] for t_i in t] # The target must be a list of the same length of the output layer (1 in this case)\n",
        "\n",
        "lr = 0.1\n",
        "steps = 1000\n",
        "out = HiddenLayer(3,1) # Perceptron\n",
        "network = [out]\n",
        "\n",
        "network = train(network, data, t, lr, steps)\n",
        "print('Expected:')\n",
        "for d in data:\n",
        "  print(d, '->', OR(*d))\n",
        "\n",
        "print('\\nPredicted:')\n",
        "for d in data:\n",
        "  res = forward(network, d)\n",
        "  print(d, '->', 0 if res[0] < 0.5 else 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmCi2oT_qf3P",
        "outputId": "1d01b85d-a24a-4d6c-82e0-d99e3272b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning started\n",
            "Learning ended\n",
            "\n",
            "Expected:\n",
            "[0 0 0] -> 0\n",
            "[0 0 1] -> 1\n",
            "[0 1 0] -> 1\n",
            "[0 1 1] -> 1\n",
            "[1 0 0] -> 1\n",
            "[1 0 1] -> 1\n",
            "[1 1 0] -> 1\n",
            "[1 1 1] -> 1\n",
            "\n",
            "Predicted:\n",
            "[0 0 0] -> 0\n",
            "[0 0 1] -> 1\n",
            "[0 1 0] -> 1\n",
            "[0 1 1] -> 1\n",
            "[1 0 0] -> 1\n",
            "[1 0 1] -> 1\n",
            "[1 1 0] -> 1\n",
            "[1 1 1] -> 1\n"
          ]
        }
      ]
    }
  ]
}